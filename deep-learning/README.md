# Deep Learning
## Paper
1. [A Learning Algorithm for Boltzmann Machines][0],DAVID H. ACKLEY,GEOFFREY E. HINTON,TERRENCE J. SEJNOWSKI,[COGNITIVE SCIENCE 9, 147-169(1985)][100]
1. [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling][19],Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, Yoshua Bengio,[11 Dec 2014][119]

## Note
1. [深度学习][50]
1. [Fundamentals of Deep Learning][20]
    - [Chapter 1. The Neural Network][21]
        - The Neuron
        - Feed-Forward Neural Networks
        - Linear Neurons
        - Sigmoid, Tanh, and ReLU Neurons
    - [Chapter 2. Training Feed-Forward Neural Networks][21]
        - The Fast-Food Problem
        - Gradient Descent
        - The Delta Rule and Learning Rates
        - Gradient Descent with Sigmoidal Neurons
        - The Backpropagation Algorithm
        - Stochastic and Minibatch Gradient Descent
        - Test Sets, Validation Sets, and Overfitting
        - Preventing Overfitting in Deep Neural Networks
    - [Chapter 3. Implementing Neural Networks in TensorFlow][22]
        - Installing TensorFlow
        - Creating and Manipulating TensorFlow Variables
        - TensorFlow Operations
        - Placeholder Tensors
        - Sessions in TensorFlow
        - Navigating Variable Scopes and Sharing Variables
        - Managing Models over the CPU and GPU
        - Specifying the Logistic Regression Model in TensorFlow
        - Logging and Training the Logistic Regression Model
        - Leveraging TensorBoard to Visualize Computation Graphs and Learning
        - Building a Multilayer Model for MNIST in TensorFlow
    - [Chapter 4. Beyond Gradient Descent][23]
        - Local Minima in the Error Surfaces of Deep Networks
        - How Pesky Are Spurious Local Minima in Deep Networks?
        - Flat Regions in the Error Surface
        - When the Gradient Points in the Wrong Direction
        - Momentum-Based Optimization
        - A Brief View of Second-Order Methods
            - Conjugate Gradient Descent
            - Broyden–Fletcher–Goldfarb–Shanno (BFGS)
        - Learning Rate Adaptation
            - AdaGrad—Accumulating Historical Gradients
            - RMSProp—Exponentially Weighted Moving Average of Gradients
            - Adam—Combining Momentum and RMSProp
        - Optimization Algorithms Experiment
    - [Chapter 5. Convolutional Neural Networks][24]
        - Vanilla Deep Neural Networks Don’t Scale
        - Filters and Feature Maps
        - Full Description of the Convolutional Layer
        - Max Pooling
        - Full Architectural Description of Convolution Networks
        - Closing the Loop on MNIST with Convolutional Networks
        - Building a Convolutional Network for CIFAR-10
        - Visualizing Learning in Convolutional Networks
        - Leveraging Convolutional Filters to Replicate Artistic Styles
    - [Chapter 6. Embedding and Representation Learning][25]
        - Learning Lower-Dimensional Representations
        - Principal Component Analysis
        - Motivating the Autoencoder Architecture
        - Implementing an Autoencoder in TensorFlow
        - Denoising to Force Robust Representations
        - Sparsity in Autoencoders
        - When Context Is More Informative than the Input Vector
        - The Word2Vec Framework
    - [Chapter 7. Models for Sequence Analysis][26]
        - Analyzing Variable-Length Inputs
        - Tackling seq2seq with Neural N-Grams
        - Implementing a Part-of-Speech Tagger
        - Dependency Parsing and SyntaxNet
        - Beam Search and Global Normalization
        - A Case for Stateful Deep Learning Models
        - Recurrent Neural Networks
        - Long Short-Term Memory (LSTM) Units
        - Implementing a Sentiment Analysis Model
        - Solving seq2seq Tasks with Recurrent Neural Networks
        - Augmenting Recurrent Networks with Attention
    - [Chapter 8. Memory Augmented Neural Networks][27]
        - Neural Turing Machines
        - Attention-Based Memory Access
        - Differentiable Neural Computers
        - Interference-Free Writing in DNCs
        - DNC Memory Reuse
        - Temporal Linking of DNC Writes
        - Understanding the DNC Read Head
        - The DNC Controller Network
    - [Chapter 9. Deep Reinforcement Learning][28]
        - Deep Reinforcement Learning Masters Atari Games
        - What Is Reinforcement Learning?
        - Markov Decision Processes (MDP)
            - Policy
            - Future Return
            - Discounted Future Return
        - Policy Versus Value Learning
        - Q-Learning and Deep Q-Networks
1. [Hands-On Machine Learning with Scikit-Learn and TensorFlow][30]


[0]: A-Learning-Algorithm-for-Boltzmann-Machines.ipynb

[20]: Fundamentals-of-Deep-Learning/
[21]: Fundamentals-of-Deep-Learning/Fundamentals-of-Deep-Learning-1+2.ipynb
[22]: Fundamentals-of-Deep-Learning/Fundamentals-of-Deep-Learning-3.ipynb
[23]: Fundamentals-of-Deep-Learning/Fundamentals-of-Deep-Learning-4.ipynb
[24]: Fundamentals-of-Deep-Learning/Fundamentals-of-Deep-Learning-5.ipynb
[25]: Fundamentals-of-Deep-Learning/Fundamentals-of-Deep-Learning-6.ipynb
[26]: Fundamentals-of-Deep-Learning/Fundamentals-of-Deep-Learning-7.ipynb
[27]: Fundamentals-of-Deep-Learning/Fundamentals-of-Deep-Learning-8.ipynb
[28]: Fundamentals-of-Deep-Learning/Fundamentals-of-Deep-Learning-9.ipynb

[30]: Hands-OnMachineLearningWithScikit-LearnAndTensorFlow/

[50]: DeepLearning/

[19]: Empirical-Evaluation-of-Gated-Recurrent-Neural-Networks-on-Sequence-Modeling.ipynb
[100]:http://www.cs.toronto.edu/~fritz/absps/cogscibm.pdf
[119]:https://arxiv.org/abs/1412.3555
